{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb9f3736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ò—Å—Ö–æ–¥–Ω–∞—è –º–æ–¥–µ–ª—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞\n",
      "üöÄ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞...\n",
      "‚úÖ –ú–æ–¥–µ–ª—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ model_optim.onnx\n",
      "‚öôÔ∏è –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ö–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ model_quant.onnx\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnxruntime import InferenceSession\n",
    "from onnxruntime.transformers.optimizer import optimize_model\n",
    "from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType\n",
    "import numpy as np\n",
    "\n",
    "model_path = \"model.onnx\"\n",
    "optimized_path = \"model_optim.onnx\"\n",
    "quantized_path = \"model_quant.onnx\"\n",
    "\n",
    "# === 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏ ===\n",
    "onnx_model = onnx.load(model_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"‚úÖ –ò—Å—Ö–æ–¥–Ω–∞—è –º–æ–¥–µ–ª—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞\")\n",
    "\n",
    "# === 2. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞ ===\n",
    "print(\"üöÄ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞...\")\n",
    "opt_model = optimize_model(model_path, model_type='bert', num_heads=0, hidden_size=0)\n",
    "opt_model.save_model_to_file(optimized_path)\n",
    "print(f\"‚úÖ –ú–æ–¥–µ–ª—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {optimized_path}\")\n",
    "\n",
    "# === 3. –ö–∞–ª–∏–±—Ä–æ–≤–æ—á–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö ===\n",
    "class DummyDataReader(CalibrationDataReader):\n",
    "    def __init__(self, input_name, num_samples=5, shape=(1, 3, 64, 64)):\n",
    "        self.input_name = input_name\n",
    "        self.data = iter([ {input_name: np.random.randn(*shape).astype(np.float32)} for _ in range(num_samples) ])\n",
    "    def get_next(self):\n",
    "        return next(self.data, None)\n",
    "\n",
    "# === 4. –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (—Å—Ç–∞—Ç–∏—á–µ—Å–∫–∞—è, –±–µ–∑ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π) ===\n",
    "print(\"‚öôÔ∏è –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é...\")\n",
    "sess = InferenceSession(optimized_path)\n",
    "input_name = sess.get_inputs()[0].name\n",
    "dr = DummyDataReader(input_name)\n",
    "\n",
    "quantize_static(\n",
    "    model_input=optimized_path,\n",
    "    model_output=quantized_path,\n",
    "    calibration_data_reader=dr,\n",
    "    weight_type=QuantType.QInt8,\n",
    ")\n",
    "print(f\"‚úÖ –ö–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {quantized_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f520b538",
   "metadata": {},
   "source": [
    "### –£–∑–Ω–∞–µ–º –∏–º–µ–Ω–∞ –≤—Ö–æ–¥–æ–≤ –∏ –≤—ã—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a687a415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input']\n",
      "['output']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "sess = onnxruntime.InferenceSession(\"./model_repository/model_onnx/1/model.onnx\")\n",
    "print([i.name for i in sess.get_inputs()])\n",
    "print([o.name for o in sess.get_outputs()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54e3bb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input']\n",
      "['output']\n"
     ]
    }
   ],
   "source": [
    "sess = onnxruntime.InferenceSession(\"./model_repository/model_optim_onnx/1/model_optim.onnx\")\n",
    "print([i.name for i in sess.get_inputs()])\n",
    "print([o.name for o in sess.get_outputs()])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
